{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f07aeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "\n",
    "from torchmetrics import __version__ as torchmetrics_version\n",
    "from pkg_resources import parse_version\n",
    "\n",
    "from torchmetrics import Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d381559a",
   "metadata": {},
   "source": [
    "### Lightning class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b8d69801",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(pl.LightningModule):\n",
    "    def __init__(self, image_shape = (1,28,28), hidden_units = (32,16)):\n",
    "        super().__init__()\n",
    "        self.train_acc = Accuracy(task = \"multiclass\", num_classes = 10)\n",
    "        self.valid_acc = Accuracy(task = \"multiclass\", num_classes = 10)\n",
    "        self.test = Accuracy(task = \"multiclass\", num_classes = 10)\n",
    "        \n",
    "        input_size = image_shape[0]*image_shape[1]*image_shape[2]\n",
    "        all_layers = [nn.Flatten()]\n",
    "        \n",
    "        for hidden_unit in hidden_units:\n",
    "            layer = nn.Linear(input_size, hidden_unit)\n",
    "            all_layers.append(layer)\n",
    "            all_layers.append(nn.ReLU())\n",
    "            input_size = hidden_unit\n",
    "        \n",
    "        all_layers.append(nn.Linear(hidden_units[-1], 10))\n",
    "        self.model = nn.Sequential(*all_layers)\n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x,y = batch\n",
    "        logits = self(x)\n",
    "        loss = nn.functional.cross_entropy(logits,y)\n",
    "        preds = torch.argmax(logits, dim = 1 )\n",
    "        self.train_acc.update(preds, y)\n",
    "        self.log(\"train_loss\", loss, prog_bar = True)\n",
    "        return loss\n",
    "    \n",
    "    def train_epoch_end(self,outs):\n",
    "        self.log(\"train_acc\", self.train_acc.compute())\n",
    "        self.train_acc.reset()\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = nn.functional.cross_entropy(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.valid_acc.update(preds, y)\n",
    "        self.log(\"valid_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def on_validation_epoch_end(self, outs):\n",
    "        self.log(\"valid_acc\", self.valid_acc.compute(), prog_bar=True)\n",
    "        self.valid_acc.reset()\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = nn.functional.cross_entropy(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.test_acc.update(preds, y)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True)\n",
    "        self.log(\"test_acc\", self.test_acc.compute(), prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "        return optimizer\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "81a14e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiLayerPerceptron()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7b4fa5e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiLayerPerceptron(\n",
       "  (train_acc): MulticlassAccuracy()\n",
       "  (valid_acc): MulticlassAccuracy()\n",
       "  (test): MulticlassAccuracy()\n",
       "  (model): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=784, out_features=32, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Linear(in_features=16, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "97312db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(10,1*28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ff415b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c1a4cdb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3420, 0.2535, 0.7935],\n",
       "        [0.0587, 0.8880, 0.5211]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c7fa179b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 1])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(x, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dab50658",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    " \n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de956ef1",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eb7d55b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistDataModule(pl.LightningDataModule):\n",
    "    def __init__(self,data_path = \"./\"):\n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "      \n",
    "    def prepare_data(self):\n",
    "        MNIST(root  = self.data_path, download = True)\n",
    "    \n",
    "    def setup(self, stage= None):\n",
    "        mnist_all = MNIST(\n",
    "        root = self.data_path,\n",
    "            train = True,\n",
    "            transform = self.transform,\n",
    "            download = False\n",
    "        )\n",
    "        \n",
    "        self.train, self.val = random_split(\n",
    "        mnist_all, [55000, 5000], generator = torch.Generator().manual_seed(1)\n",
    "        )\n",
    "        \n",
    "        self.test = MNIST(\n",
    "        root = self.data_path,\n",
    "        train = False,\n",
    "            transform = self.transform,\n",
    "            download= False\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train, batch_size = 64, num_workers = 4)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val, batch_size=64, num_workers=4)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test, batch_size=64, num_workers=4)\n",
    "  \n",
    "\n",
    "torch.manual_seed(1)\n",
    "mnist_dm = MnistDataModule()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "89520554",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2f4cc2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 9912422/9912422 [00:19<00:00, 511998.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 28881/28881 [00:00<00:00, 5089093.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 1648877/1648877 [00:02<00:00, 556378.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 4542/4542 [00:00<00:00, 4606027.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | train_acc | MulticlassAccuracy | 0     \n",
      "1 | valid_acc | MulticlassAccuracy | 0     \n",
      "2 | test      | MulticlassAccuracy | 0     \n",
      "3 | model     | Sequential         | 25.8 K\n",
      "-------------------------------------------------\n",
      "25.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.8 K    Total params\n",
      "0.103     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:04<00:00,  2.41s/it]"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "MultiLayerPerceptron.on_validation_epoch_end() missing 1 required positional argument: 'outs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [ModelCheckpoint(save_top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m, monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid_acc\u001b[39m\u001b[38;5;124m\"\u001b[39m)] \u001b[38;5;66;03m# save top 1 model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(max_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m, callbacks \u001b[38;5;241m=\u001b[39m callbacks)\n\u001b[0;32m----> 6\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(model \u001b[38;5;241m=\u001b[39m mnistclassifier, datamodule \u001b[38;5;241m=\u001b[39m mnist_dm)\n",
      "File \u001b[0;32m~/anaconda3/envs/learnings/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:529\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    527\u001b[0m model \u001b[38;5;241m=\u001b[39m _maybe_unwrap_optimized(model)\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m--> 529\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_and_handle_interrupt(\n\u001b[1;32m    530\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    531\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/learnings/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:42\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     45\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/anaconda3/envs/learnings/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:568\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_connector\u001b[38;5;241m.\u001b[39mattach_data(\n\u001b[1;32m    559\u001b[0m     model, train_dataloaders\u001b[38;5;241m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[38;5;241m=\u001b[39mval_dataloaders, datamodule\u001b[38;5;241m=\u001b[39mdatamodule\n\u001b[1;32m    560\u001b[0m )\n\u001b[1;32m    562\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    564\u001b[0m     ckpt_path,\n\u001b[1;32m    565\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    566\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    567\u001b[0m )\n\u001b[0;32m--> 568\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(model, ckpt_path\u001b[38;5;241m=\u001b[39mckpt_path)\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/learnings/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:973\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    970\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 973\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_stage()\n\u001b[1;32m    975\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    977\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    978\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/learnings/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1014\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m   1013\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1014\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1015\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1016\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/anaconda3/envs/learnings/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1043\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1040\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1043\u001b[0m val_loop\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m   1045\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/learnings/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:177\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loop_run(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/learnings/lib/python3.11/site-packages/pytorch_lightning/loops/evaluation_loop.py:122\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store_dataloader_outputs()\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_run_end()\n",
      "File \u001b[0;32m~/anaconda3/envs/learnings/lib/python3.11/site-packages/pytorch_lightning/loops/evaluation_loop.py:244\u001b[0m, in \u001b[0;36m_EvaluationLoop.on_run_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39m_logger_connector\u001b[38;5;241m.\u001b[39m_evaluation_epoch_end()\n\u001b[1;32m    243\u001b[0m \u001b[38;5;66;03m# hook\u001b[39;00m\n\u001b[0;32m--> 244\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_evaluation_epoch_end()\n\u001b[1;32m    246\u001b[0m logged_outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logged_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logged_outputs, []  \u001b[38;5;66;03m# free memory\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# include any logged outputs on epoch_end\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/learnings/lib/python3.11/site-packages/pytorch_lightning/loops/evaluation_loop.py:326\u001b[0m, in \u001b[0;36m_EvaluationLoop._on_evaluation_epoch_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    324\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_test_epoch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_validation_epoch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    325\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(trainer, hook_name)\n\u001b[0;32m--> 326\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_lightning_module_hook(trainer, hook_name)\n\u001b[1;32m    328\u001b[0m trainer\u001b[38;5;241m.\u001b[39m_logger_connector\u001b[38;5;241m.\u001b[39mon_epoch_end()\n",
      "File \u001b[0;32m~/anaconda3/envs/learnings/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:144\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 144\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    147\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "\u001b[0;31mTypeError\u001b[0m: MultiLayerPerceptron.on_validation_epoch_end() missing 1 required positional argument: 'outs'"
     ]
    }
   ],
   "source": [
    "mnistclassifier = MultiLayerPerceptron()\n",
    "\n",
    "callbacks = [ModelCheckpoint(save_top_k=1, mode='max', monitor=\"valid_acc\")] # save top 1 model\n",
    "\n",
    "trainer = pl.Trainer(max_epochs = 10, callbacks = callbacks)\n",
    "trainer.fit(model = mnistclassifier, datamodule = mnist_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "df7135a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9a2ab5bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'python_environment_check'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpython_environment_check\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_packages\n\u001b[1;32m      4\u001b[0m d \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1.8\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorchvision\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0.9.0\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorchmetrics\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0.6.2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     10\u001b[0m }\n\u001b[1;32m     11\u001b[0m check_packages(d)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'python_environment_check'"
     ]
    }
   ],
   "source": [
    "from python_environment_check import check_packages\n",
    "\n",
    "\n",
    "d = {\n",
    "    'torch': '1.8',\n",
    "    'torchvision': '0.9.0',\n",
    "    'tensorboard': '2.7.0',\n",
    "    'pytorch_lightning': '1.5.0',\n",
    "    'torchmetrics': '0.6.2'\n",
    "}\n",
    "check_packages(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3c35ea76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep_mechanics_of_pytorch.ipynb\r\n",
      "\u001b[34mMNIST\u001b[m\u001b[m/\r\n",
      "Pytorch Lightning.ipynb\r\n",
      "\u001b[34mcat_dog_images\u001b[m\u001b[m/\r\n",
      "\u001b[34mlightning_logs\u001b[m\u001b[m/\r\n",
      "predicting_fuel_efficiency_of_car.ipynb\r\n",
      "pytorch_basics_and_simple_multilayer_perceptron.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5eff2b9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pytorch_lightning' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pytorch_lightning \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m-\u001b[39mversion\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pytorch_lightning' is not defined"
     ]
    }
   ],
   "source": [
    "pytorch_lightning --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2fe8aaf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.5'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209bbd4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learnings",
   "language": "python",
   "name": "learnings"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
